# -*- coding: utf-8 -*-
"""Feature Engineering Lab

Automatically generated by Colaboratory.

"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix
from sklearn.datasets import load_iris
import pandas as pd
import numpy as np

all = load_iris()

X = all.data
y = all.target

X.shape, y.shape

all.target_names

y

def cv_score(X,y=y, folds=5):
  print ("Dataset shape: ", X.shape)
  train_avg_score = []
  test_avg_score = []

  kf_cross_validator = KFold (n_splits=folds, shuffle=True, random_state=1)
  
  for train_idx, test_idx in kf_cross_validator.split(X):
    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx],  y[train_idx], y[test_idx]
    #print (X_train.shape, X_test.shape)
    logit = LogisticRegression(max_iter=3_000)
    logit.fit(X_train, y_train)
    y_pred = logit.predict(X_test)
    #print (confusion_matrix(y_test, y_pred))
    train_avg_score.append(logit.score(X_train, y_train))
    test_avg_score.append(logit.score(X_test,y_test))

  print ("Average training score: %.3f, average test score: %.3f" % (np.mean(train_avg_score), np.mean(test_avg_score)))
  print ("-----------------------------------------------------------")

cv_score(X)

"""### Packaging data in a pandas DataFrame"""



fn = all.feature_names; fn

fn = ["_".join(f.split()[:2]) + "_cm" for f in fn]; fn

df = pd.DataFrame(all.data, columns=fn)
df.shape

df.head(3)

df.corr()

df['iris_class'] = y
df.info()

sns.set()
sns.pairplot(df, hue='iris_class');

plt.figure(figsize=  (12,8))
sns.scatterplot (x = 'sepal_length_cm', y = 'sepal_width_cm', \
                 data = df, hue = 'iris_class', palette='bright', \
                 size = 'petal_width_cm', \
                 sizes=(20, 500), alpha = 0.7 );

"""Using 3 features for training: dropping the sepal_length column"""

df.columns

df.iloc[:,[1,2,3]].head(3)

cv_score(np.array(df.iloc[:,[1,2,3]]))

"""Using two features: petal_length and petal_width are strongly correlated -- dropping petal_width"""

cv_score(np.array(df.iloc[:,[1,2]]))

"""### Feature Engineering: multiply petal_width by petal_length"""

type(X)

X_mix = np.column_stack((X[:,1], X[:,2] *  X[:,3]))
print (X_mix[:3])

cv_score(X_mix)
