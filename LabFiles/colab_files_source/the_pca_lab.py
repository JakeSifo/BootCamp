# -*- coding: utf-8 -*-
"""The PCA Lab

Automatically generated by Colaboratory.

"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.metrics import mean_absolute_error
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
import xgboost as xgb

file_url = 'http://bit.ly/housing_prices_csv'

df = pd.read_csv(file_url, index_col='Id')
df.shape
#df_copy = df.copy()

"""## Getting information on numeric columns that have NaNs"""

import io
buffer = io.StringIO()
df.info(buf = buffer)
df_info = buffer.getvalue() 
df_info_list = df_info.split('\n')

no_nans = str(df.shape[0])
for x in df_info_list:
  if 'object' not in x:
    if no_nans not in x:
      print (x)

"""### One-Hot Encoding and NaN fixes"""

lf_col = df.LotFrontage
lf_col.fillna(lf_col.mean(), inplace = True)

mas_col = df.MasVnrArea
mas_col.fillna(mas_col.mean(), inplace = True)

df.GarageYrBlt.describe()

gr_yr_col = df.GarageYrBlt
gr_yr_col.fillna(gr_yr_col.mean(), inplace = True)
df.GarageYrBlt = preprocessing.MinMaxScaler().\
fit_transform(np.array(df.GarageYrBlt).reshape(-1,1))

df.GarageYrBlt.describe()

df = pd.get_dummies(df)
len(df.columns)

"""### Get the feature and label datasets"""

y = df.SalePrice  
X = df.drop(['SalePrice'], axis=1)
X.shape

"""### Scale the Feature Dataset"""

X = preprocessing.MinMaxScaler(feature_range=(0,1)).\
                   fit_transform(X)

X[:5,:5]

"""## Split X and y into training and test datasets"""

X_train, X_test, y_train, y_test = train_test_split(X, y, \
 test_size=0.2, random_state=0)

"""### PCA"""

pca = PCA (random_state=1)
pca.fit(X_train)
len(pca.components_)

num_comp = 21
pca = PCA (n_components=num_comp, random_state=1)
pca.fit(X_train)

pca.components_.shape

pca.explained_variance_

np.cumsum(pca.explained_variance_ratio_)

plt.figure(figsize= (10,10))
plt.grid(True)
plt.plot( pca.explained_variance_, 'ro-', linewidth=2);

X_pca_train = pca.transform (X_train) 
X_pca_test = pca.transform (X_test)
X_pca_train.shape, X_train.shape, X_pca_test.shape, X_test.shape

type(X_train), type(X_pca_train)

"""### Use the PCA-compressed datasets in Regression"""

xgb_model = xgb.XGBRegressor (objective='reg:squarederror', random_state=777)

# Commented out IPython magic to ensure Python compatibility.
# %timeit xgb_model.fit(X_pca_train, y_train)

predictions = xgb_model.predict(X_pca_test)

print ("The MEA: " + str(mean_absolute_error(predictions, y_test)))

xgb_model.score (X_pca_train, y_train), xgb_model.score (X_pca_test, y_test)

"""## Double the number of PCA components"""

num_comp = 2 * num_comp

pca = PCA (n_components=num_comp, random_state=1)
pca.fit(X_train)
X_pca_train = pca.transform (X_train)
X_pca_test = pca.transform (X_test)

#%timeit X_pca_test = pca.transform (X_test)

X_pca_train.shape, X_train.shape, X_pca_test.shape, X_test.shape

xgb_model = xgb.XGBRegressor (objective='reg:squarederror', random_state=777)

# Commented out IPython magic to ensure Python compatibility.

# %timeit xgb_model.fit(X_pca_train, y_train)

predictions = xgb_model.predict(X_pca_test)
print ("The MEA: " + str(mean_absolute_error(predictions, y_test)))
xgb_model.score(X_pca_train,y_train),xgb_model.score (X_pca_test, y_test)

print ('Done ...')

