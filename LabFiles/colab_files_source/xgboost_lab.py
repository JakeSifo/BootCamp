# -*- coding: utf-8 -*-
"""The XGBoost Lab

Automatically generated by Colaboratory.

"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import xgboost as xgb

xgb.__version__

!wget bit.ly/housing_data -O housing_prices_data.zip

!unzip housing_prices_data.zip

!cat   housing_prices_data_description.txt

data = "housing_prices_data.csv"
XAll = pd.read_csv(data, index_col='Id')
XAll.shape

XAll.columns

XAll[:10]

XAll.info()

type(XAll.isnull().sum())

ser = XAll.isnull().sum()
for i in ser.items():
  print (i)

ser = XAll.isna().sum()
for i in ser.items():
  print (i)

df = XAll.copy()

df.LotFrontage[:10]

df.LotFrontage.describe()

lf_col = df.LotFrontage
lf_col .fillna(lf_col.mean(), inplace = True)
lf_col.describe()

df.LotFrontage[:10]

"""### Apply the One-Hot Encoding Scheme"""

for c in df.columns:
  col = df.loc[:,c]
  if col.dtype == np.dtype('O'):
    print (col.name, col.dtype, col.unique())

c_PoolQC = df.PoolQC
c_PoolQC.describe()

c_PoolQC.unique()

c_PoolQC.isna().sum()

c_PoolQC.loc[c_PoolQC == 'Gd'].count()

c_PoolQC.loc[c_PoolQC == 'Ex']

c_PoolQC[:10]

c_PoolQC_enc = pd.get_dummies(c_PoolQC)

c_PoolQC_enc.describe()

c_PoolQC_enc[:10]

type(c_PoolQC_enc)

df = pd.get_dummies(df)

df.columns

df.describe()

"""### Create the Training and Test Datasets"""

y = df.SalePrice

X = df.drop(['SalePrice'], axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, \
 train_size=0.8, test_size=0.2, random_state=0)



print (X_train.shape, y_train.shape)
print (X_test.shape, y_test.shape)

"""### Create and Train the Model """

xgb_model = xgb.XGBRegressor (objective='reg:squarederror', \
 random_state=777)

xgb_model.fit(X_train, y_train)

predictions = xgb_model.predict(X_test)

from sklearn.metrics import mean_absolute_error
print ("The MEA: " + str(mean_absolute_error(predictions, y_test)))

xgb_model.score (X_train, y_train), xgb_model.score (X_test, y_test)

xgb_model2 = xgb.XGBRegressor (objective='reg:squarederror', \
 random_state=777, n_estimators=200)

xgb_model2.fit(X_train, y_train)
predictions2 = xgb_model2.predict(X_test)
print ("The MEA: " + str(mean_absolute_error(predictions2, y_test)))
xgb_model2.score (X_train, y_train), xgb_model2.score (X_test, y_test)

"""### Finding the Most Important (Statistically Significant) Features"""

xgb_model.feature_importances_

sum(xgb_model.feature_importances_)

xgb_model.feature_importances_.shape

xgb_model.feature_importances_?

fi = xgb_model.feature_importances_
col_fi = [(c,f) for c,f in zip(df.columns, fi)]; col_fi[:5]

sorted_cl_fi = sorted(col_fi, key = lambda kv: kv[1], reverse=True )
sorted_cl_fi[:7]

sorted_cl_fi [-10:]

