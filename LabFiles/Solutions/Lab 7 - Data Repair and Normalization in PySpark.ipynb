{"cells":[{"cell_type":"code","source":["df_with_nulls = spark.createDataFrame([(None, None), (1.0, None), (float('nan'), 2.0), (11.0, 22.0)], (\"A\", \"B\"))\ndf_with_nulls.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d6afc527-2c09-46f1-91db-0be9aeb2996a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+----+\n|   A|   B|\n+----+----+\n|null|null|\n| 1.0|null|\n| NaN| 2.0|\n|11.0|22.0|\n+----+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df_with_nulls.dropna(how='all').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1d175086-1982-4864-b680-a498b0cc37f1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+----+\n|   A|   B|\n+----+----+\n| 1.0|null|\n| NaN| 2.0|\n|11.0|22.0|\n+----+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df_with_nulls.dropna(how='any').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"f2b77ade-1546-4581-8ae5-817e2e9f2772","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+----+\n|   A|   B|\n+----+----+\n|11.0|22.0|\n+----+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df_with_nulls.na.fill(0).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"235049f3-0ec8-4936-91ab-d2b6d2f11416","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+----+\n|   A|   B|\n+----+----+\n| 0.0| 0.0|\n| 1.0| 0.0|\n| 0.0| 2.0|\n|11.0|22.0|\n+----+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import Imputer\nimputer = Imputer(inputCols=[\"A\", \"B\"], outputCols=[\"out_a\", \"out_b\"])\nmodel = imputer.fit(df_with_nulls)  \ndf_mean = model.transform(df_with_nulls)  \ndf_mean.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"898c4233-ac47-41c0-9f7a-bdcca12e9e58","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+----+-----+-----+\n|   A|   B|out_a|out_b|\n+----+----+-----+-----+\n|null|null|  6.0| 12.0|\n| 1.0|null|  1.0| 12.0|\n| NaN| 2.0|  6.0|  2.0|\n|11.0|22.0| 11.0| 22.0|\n+----+----+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df_mean = df_mean.select ('out_a', 'out_b')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"c47cd5f5-955e-4183-b46f-530058d2513a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.linalg import DenseVector\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import Normalizer\nfrom math import *"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"39ea05b1-0462-4f95-ae39-1dbb6f5eafc9","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["veca = VectorAssembler(inputCols = ['out_a', 'out_b'], \\\noutputCol = 'feature_vector')\ndf_feat = veca.transform(df_mean)\ndf_feat.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e6533317-f1be-412b-86a1-42f4345b8092","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----+--------------+\n|out_a|out_b|feature_vector|\n+-----+-----+--------------+\n|  6.0| 12.0|    [6.0,12.0]|\n|  1.0| 12.0|    [1.0,12.0]|\n|  6.0|  2.0|     [6.0,2.0]|\n| 11.0| 22.0|   [11.0,22.0]|\n+-----+-----+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["normalizer = Normalizer(inputCol=\"feature_vector\", outputCol=\"features\")\nnormalizer.transform(df_feat).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"9cf1883f-355f-44e9-acf5-de62cda42087","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[9]: [Row(out_a=6.0, out_b=12.0, feature_vector=DenseVector([6.0, 12.0]), features=DenseVector([0.4472, 0.8944])),\n Row(out_a=1.0, out_b=12.0, feature_vector=DenseVector([1.0, 12.0]), features=DenseVector([0.083, 0.9965])),\n Row(out_a=6.0, out_b=2.0, feature_vector=DenseVector([6.0, 2.0]), features=DenseVector([0.9487, 0.3162])),\n Row(out_a=11.0, out_b=22.0, feature_vector=DenseVector([11.0, 22.0]), features=DenseVector([0.4472, 0.8944]))]"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"da31cff8-830a-43fc-8694-c5d8f772a0fc","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Data Repair and Normalization in PySpark Lab","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
